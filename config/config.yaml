# Configuration for Toxicity Detection System

# Rule-based detector settings
rule_based:
  # Toxic keywords and patterns
  toxic_keywords:
    - "hate"
    - "stupid"
    - "idiot"
    - "kill"
    - "die"

  offensive_keywords:
    - "nigga"
    - "nigger"
    - "niggers"
    - "niggas"
    - "retards"
    - "retard"
    - "autistic"
    - "homo"
    - "homosexual"
    - "gay"
    - "lesbian"
    - "homo"
    - "fuck"
    - "shit"
    - "cum"
    - "faggot"
    - "fag"
    - "cuck"
    - "whore"
    - "slut"
    - "bitch"
  
  # Context-aware patterns (word combinations that become toxic in context)
  context_patterns:
    - ["you", "should", "die"]
    - ["go", "kill", "yourself"]
    - ["nobody", "likes", "you"]
    - ["kill", "yourself"]

  
  # Severity weights
  severity_weights:
    high: 1.0
    medium: 0.6
    low: 0.3

# ML model settings
ml_model:
  # Model type: 'logistic_regression', 'naive_bayes', 'random_forest', 'svm'
  model_type: 'logistic_regression'
  
  # Feature extraction
  vectorizer: 'tfidf'  # 'tfidf' or 'count'
  max_features: 5000
  ngram_range: [1, 3]  # unigrams to trigrams
  
  # Training parameters
  test_size: 0.2
  random_state: 42
  
  # Model paths
  model_path: 'models/toxicity_model.joblib'
  vectorizer_path: 'models/vectorizer.joblib'

# Data paths
data:
  raw_data: 'data/raw/toxic_comments.csv'
  processed_data: 'data/processed/processed_data.csv'
  sample_data: 'data/sample_dataset.csv'

# Evaluation settings
evaluation:
  metrics:
    - 'accuracy'
    - 'precision'
    - 'recall'
    - 'f1_score'
  
  # Context window for analysis
  context_window: 3  # number of sentences to consider for context
